{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# Â§öË°åËæìÂá∫\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[huggingface/pytorch-transformers: üëæ A library of state-of-the-art pretrained models for Natural Language Processing (NLP)](https://github.com/huggingface/pytorch-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_transformers import *\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('/home/lyc/.torch/models/bert-base-uncased/')\n",
    "# os.makedirs(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT ÁªìÊûÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/lyc/.torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/lyc/.torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)  ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑËØçÂÖ∏\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'who',\n",
       " 'was',\n",
       " 'jim',\n",
       " 'henson',\n",
       " '?',\n",
       " '[SEP]',\n",
       " 'jim',\n",
       " 'henson',\n",
       " 'was',\n",
       " 'a',\n",
       " 'puppet',\n",
       " '##eer',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'who',\n",
       " 'was',\n",
       " 'jim',\n",
       " 'henson',\n",
       " '?',\n",
       " '[SEP]',\n",
       " 'jim',\n",
       " '[MASK]',\n",
       " 'was',\n",
       " 'a',\n",
       " 'puppet',\n",
       " '##eer',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2040,\n",
       " 2001,\n",
       " 3958,\n",
       " 27227,\n",
       " 1029,\n",
       " 102,\n",
       " 3958,\n",
       " 103,\n",
       " 2001,\n",
       " 1037,\n",
       " 13997,\n",
       " 11510,\n",
       " 102]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_tokens  # Á¥¢Âºï stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2040,  2001,  3958, 27227,  1029,   102,  3958,   103,  2001,\n",
       "          1037, 13997, 11510,   102]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "tokens_tensor\n",
    "segments_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/lyc/.torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache, downloading to /tmp/tmp753pfojq\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 440473133/440473133 [02:09<00:00, 3404562.28B/s]\n",
      "INFO:pytorch_transformers.file_utils:copying /tmp/tmp753pfojq to cache at /home/lyc/.torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:pytorch_transformers.file_utils:creating metadata file for /home/lyc/.torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmp753pfojq\n",
      "INFO:pytorch_transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/lyc/.torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased', cached_dir=root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model in evaluation mode to desactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproductible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    # See the models docstrings for the detail of the inputs\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    # PyTorch-Transformers models always output tuples.\n",
    "    # See the models docstrings for the detail of all the outputs\n",
    "    # In our case, the first element is the hidden state of the last layer of the Bert model\n",
    "    encoded_layers = outputs[0]\n",
    "# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n",
    "assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)\n",
    "outputs[0].size()\n",
    "outputs[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5570,  0.2839, -0.6436,  ..., -0.7274,  0.4557,  0.6204],\n",
       "         [-1.1572,  0.0354,  0.0355,  ...,  0.0591,  0.1097, -0.3150],\n",
       "         [ 0.1008, -0.5286, -0.4688,  ..., -0.0431,  0.4889,  0.4134],\n",
       "         ...,\n",
       "         [ 0.1948,  0.0761,  0.2893,  ..., -0.0807,  0.7071,  0.0502],\n",
       "         [-0.1119,  0.0714,  0.6101,  ...,  0.4044,  0.1614, -0.3569],\n",
       "         [ 0.8296,  0.2729, -0.3090,  ...,  0.2452, -0.3581, -0.1970]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused171]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = BertForSequenceClassification(config)\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model1(input_ids, labels=labels)\n",
    "loss, logits = outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8445613384246826"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5092, 0.2263]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1].detach().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMultipleChoice(config)\n",
    "choices = [\"Hello, my dog is cute\", \"Hello, my cat is amazing\"]\n",
    "input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\n",
    "labels = torch.tensor(1).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids, labels=labels)\n",
    "loss, classification_scores = outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4529, grad_fn=<NllLossBackward>),\n",
       " tensor([[-0.4277,  0.1295]], grad_fn=<ViewBackward>))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7592,  1010,  2026,  3899,  2003, 10140],\n",
       "         [ 7592,  1010,  2026,  4937,  2003,  6429]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', ',', 'my', 'dog', 'is', 'cute'],\n",
       " ['hello', ',', 'my', 'cat', 'is', 'amazing']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.tokenize(s) for s in choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_labels\": 2,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json not found in cache, downloading to /tmp/tmpj__borob\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:00<00:00, 199620.74B/s]\n",
      "INFO:pytorch_transformers.file_utils:copying /tmp/tmpj__borob to cache at /home/lyc/.torch/pytorch_transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
      "INFO:pytorch_transformers.file_utils:creating metadata file for /home/lyc/.torch/pytorch_transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
      "INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmpj__borob\n",
      "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/lyc/.torch/pytorch_transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"token_ids\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json not found in cache, downloading to /tmp/tmp3co8diqb\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1042301/1042301 [00:02<00:00, 440190.67B/s]\n",
      "INFO:pytorch_transformers.file_utils:copying /tmp/tmp3co8diqb to cache at /home/lyc/.torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "INFO:pytorch_transformers.file_utils:creating metadata file for /home/lyc/.torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmp3co8diqb\n",
      "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt not found in cache, downloading to /tmp/tmp2nv_wasz\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 456318/456318 [00:01<00:00, 297021.31B/s]\n",
      "INFO:pytorch_transformers.file_utils:copying /tmp/tmp2nv_wasz to cache at /home/lyc/.torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "INFO:pytorch_transformers.file_utils:creating metadata file for /home/lyc/.torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmp2nv_wasz\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/lyc/.torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/lyc/.torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model(config)\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids)\n",
    "last_hidden_states = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 1, 12, 6, 64]),\n",
       " torch.Size([2, 1, 12, 6, 64]),\n",
       " torch.Size([2, 1, 12, 6, 64]),\n",
       " torch.Size([2, 1, 12, 6, 64]),\n",
       " torch.Size([2, 1, 12, 6, 64]),\n",
       " torch.Size([2, 1, 12, 6, 64]),\n",
       " torch.Size([2, 1, 12, 6, 64]),\n",
       " torch.Size([2, 1, 12, 6, 64]),\n",
       " torch.Size([2, 1, 12, 6, 64]),\n",
       " torch.Size([2, 1, 12, 6, 64]),\n",
       " torch.Size([2, 1, 12, 6, 64]),\n",
       " torch.Size([2, 1, 12, 6, 64])]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].size()\n",
    "[x.size() for x in outputs[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1)\n",
       "  (h): ModuleList(\n",
       "    (0): Block(\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): BertLayerNorm()\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT2 ‰ΩøÁî®12‰∏™transformerÁöÑdecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-config.json not found in cache, downloading to /tmp/tmprc0lzrpf\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 606/606 [00:00<00:00, 781355.13B/s]\n",
      "INFO:pytorch_transformers.file_utils:copying /tmp/tmprc0lzrpf to cache at /home/lyc/.torch/pytorch_transformers/a6dfd6a3896b3ae4c1a3c5f26ff1f1827c26c15b679de9212a04060eaf1237df.aef76fb1064c932cd6a2a2be3f23ebbfa5f9b6e29e8e87b571c45b4a5d5d1b90\n",
      "INFO:pytorch_transformers.file_utils:creating metadata file for /home/lyc/.torch/pytorch_transformers/a6dfd6a3896b3ae4c1a3c5f26ff1f1827c26c15b679de9212a04060eaf1237df.aef76fb1064c932cd6a2a2be3f23ebbfa5f9b6e29e8e87b571c45b4a5d5d1b90\n",
      "INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmprc0lzrpf\n",
      "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-config.json from cache at /home/lyc/.torch/pytorch_transformers/a6dfd6a3896b3ae4c1a3c5f26ff1f1827c26c15b679de9212a04060eaf1237df.aef76fb1064c932cd6a2a2be3f23ebbfa5f9b6e29e8e87b571c45b4a5d5d1b90\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"adaptive\": true,\n",
      "  \"attn_type\": 0,\n",
      "  \"clamp_len\": 1000,\n",
      "  \"cutoffs\": [\n",
      "    20000,\n",
      "    40000,\n",
      "    200000\n",
      "  ],\n",
      "  \"d_embed\": 1024,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 4096,\n",
      "  \"d_model\": 1024,\n",
      "  \"div_val\": 4,\n",
      "  \"dropatt\": 0.0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"ext_len\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"init\": \"normal\",\n",
      "  \"init_range\": 0.01,\n",
      "  \"init_std\": 0.02,\n",
      "  \"mem_len\": 1600,\n",
      "  \"n_head\": 16,\n",
      "  \"n_layer\": 18,\n",
      "  \"n_token\": 267735,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pre_lnorm\": false,\n",
      "  \"proj_init_std\": 0.01,\n",
      "  \"same_length\": true,\n",
      "  \"sample_softmax\": -1,\n",
      "  \"tgt_len\": 128,\n",
      "  \"tie_projs\": [\n",
      "    false,\n",
      "    true,\n",
      "    true,\n",
      "    true\n",
      "  ],\n",
      "  \"tie_weight\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"untie_r\": true\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-vocab.bin not found in cache, downloading to /tmp/tmpttbuxgn3\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9143613/9143613 [00:12<00:00, 723068.70B/s]\n",
      "INFO:pytorch_transformers.file_utils:copying /tmp/tmpttbuxgn3 to cache at /home/lyc/.torch/pytorch_transformers/b24cb708726fd43cbf1a382da9ed3908263e4fb8a156f9e0a4f45b7540c69caa.a6a9c41b856e5c31c9f125dd6a7ed4b833fbcefda148b627871d4171b25cffd1\n",
      "INFO:pytorch_transformers.file_utils:creating metadata file for /home/lyc/.torch/pytorch_transformers/b24cb708726fd43cbf1a382da9ed3908263e4fb8a156f9e0a4f45b7540c69caa.a6a9c41b856e5c31c9f125dd6a7ed4b833fbcefda148b627871d4171b25cffd1\n",
      "INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmpttbuxgn3\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-vocab.bin from cache at /home/lyc/.torch/pytorch_transformers/b24cb708726fd43cbf1a382da9ed3908263e4fb8a156f9e0a4f45b7540c69caa.a6a9c41b856e5c31c9f125dd6a7ed4b833fbcefda148b627871d4171b25cffd1\n"
     ]
    }
   ],
   "source": [
    "config = TransfoXLConfig.from_pretrained('transfo-xl-wt103')\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')\n",
    "model = TransfoXLModel(config)\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(input_ids)\n",
    "last_hidden_states, mems = outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"adaptive\": true,\n",
       "  \"attn_type\": 0,\n",
       "  \"clamp_len\": 1000,\n",
       "  \"cutoffs\": [\n",
       "    20000,\n",
       "    40000,\n",
       "    200000\n",
       "  ],\n",
       "  \"d_embed\": 1024,\n",
       "  \"d_head\": 64,\n",
       "  \"d_inner\": 4096,\n",
       "  \"d_model\": 1024,\n",
       "  \"div_val\": 4,\n",
       "  \"dropatt\": 0.0,\n",
       "  \"dropout\": 0.1,\n",
       "  \"ext_len\": 0,\n",
       "  \"finetuning_task\": null,\n",
       "  \"init\": \"normal\",\n",
       "  \"init_range\": 0.01,\n",
       "  \"init_std\": 0.02,\n",
       "  \"mem_len\": 1600,\n",
       "  \"n_head\": 16,\n",
       "  \"n_layer\": 18,\n",
       "  \"n_token\": 267735,\n",
       "  \"num_labels\": 2,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"pre_lnorm\": false,\n",
       "  \"proj_init_std\": 0.01,\n",
       "  \"same_length\": true,\n",
       "  \"sample_softmax\": -1,\n",
       "  \"tgt_len\": 128,\n",
       "  \"tie_projs\": [\n",
       "    false,\n",
       "    true,\n",
       "    true,\n",
       "    true\n",
       "  ],\n",
       "  \"tie_weight\": true,\n",
       "  \"torchscript\": false,\n",
       "  \"untie_r\": true\n",
       "}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransfoXLModel(\n",
       "  (word_emb): AdaptiveEmbedding(\n",
       "    (emb_layers): ModuleList(\n",
       "      (0): Embedding(20000, 1024)\n",
       "      (1): Embedding(20000, 256)\n",
       "      (2): Embedding(160000, 64)\n",
       "      (3): Embedding(67735, 16)\n",
       "    )\n",
       "    (emb_projs): ParameterList(\n",
       "        (0): Parameter containing: [torch.FloatTensor of size 1024x1024]\n",
       "        (1): Parameter containing: [torch.FloatTensor of size 1024x256]\n",
       "        (2): Parameter containing: [torch.FloatTensor of size 1024x64]\n",
       "        (3): Parameter containing: [torch.FloatTensor of size 1024x16]\n",
       "    )\n",
       "  )\n",
       "  (drop): Dropout(p=0.1)\n",
       "  (layers): ModuleList(\n",
       "    (0): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (1): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (2): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (3): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (4): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (5): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (6): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (7): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (8): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (9): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (10): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (11): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (12): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (13): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (14): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (15): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (16): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (17): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1)\n",
       "        (dropatt): Dropout(p=0.0)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): BertLayerNorm()\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "        )\n",
       "        (layer_norm): BertLayerNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pos_emb): PositionalEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1024])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1600, 1, 1024]),\n",
       " torch.Size([1600, 1, 1024]),\n",
       " torch.Size([1600, 1, 1024]),\n",
       " torch.Size([1600, 1, 1024]),\n",
       " torch.Size([1600, 1, 1024]),\n",
       " torch.Size([1600, 1, 1024]),\n",
       " torch.Size([1600, 1, 1024]),\n",
       " torch.Size([1600, 1, 1024]),\n",
       " torch.Size([1600, 1, 1024]),\n",
       " torch.Size([1600, 1, 1024]),\n",
       " torch.Size([1600, 1, 1024]),\n",
       " torch.Size([1600, 1, 1024]),\n",
       " torch.Size([1600, 1, 1024]),\n",
       " torch.Size([1600, 1, 1024]),\n",
       " torch.Size([1600, 1, 1024]),\n",
       " torch.Size([1600, 1, 1024]),\n",
       " torch.Size([1600, 1, 1024]),\n",
       " torch.Size([1600, 1, 1024])]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.size() for x in outputs[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json not found in cache, downloading to /tmp/tmpzhdpubiu\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 641/641 [00:00<00:00, 736992.56B/s]\n",
      "INFO:pytorch_transformers.file_utils:copying /tmp/tmpzhdpubiu to cache at /home/lyc/.torch/pytorch_transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.ef1824921bc0786e97dc88d55eb17aabf18aac90f24bd34c0650529e7ba27d6f\n",
      "INFO:pytorch_transformers.file_utils:creating metadata file for /home/lyc/.torch/pytorch_transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.ef1824921bc0786e97dc88d55eb17aabf18aac90f24bd34c0650529e7ba27d6f\n",
      "INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmpzhdpubiu\n",
      "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at /home/lyc/.torch/pytorch_transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.ef1824921bc0786e97dc88d55eb17aabf18aac90f24bd34c0650529e7ba27d6f\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_token\": 32000,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"untie_r\": true\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-large-cased-spiece.model from cache at /home/lyc/.torch/pytorch_transformers/5b125ba222ff82664771f63cd8fac9696c24b403fc1ab720d537fe2ceaaf0576.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8\n"
     ]
    }
   ],
   "source": [
    "config = XLNetConfig.from_pretrained('xlnet-base-cased')\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
    "model = XLNetModel(config)\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"attn_type\": \"bi\",\n",
       "  \"bi_data\": false,\n",
       "  \"clamp_len\": -1,\n",
       "  \"d_head\": 64,\n",
       "  \"d_inner\": 3072,\n",
       "  \"d_model\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"end_n_top\": 5,\n",
       "  \"ff_activation\": \"gelu\",\n",
       "  \"finetuning_task\": null,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"mem_len\": null,\n",
       "  \"n_head\": 12,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_token\": 32000,\n",
       "  \"num_labels\": 2,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"reuse_len\": null,\n",
       "  \"same_length\": false,\n",
       "  \"start_n_top\": 5,\n",
       "  \"summary_activation\": \"tanh\",\n",
       "  \"summary_last_dropout\": 0.1,\n",
       "  \"summary_type\": \"last\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"torchscript\": false,\n",
       "  \"untie_r\": true\n",
       "}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetModel(\n",
       "  (word_embedding): Embedding(32000, 768)\n",
       "  (layer): ModuleList(\n",
       "    (0): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (1): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (2): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (3): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (4): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (5): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (6): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (7): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (8): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (9): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (10): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (11): XLNetLayer(\n",
       "      (rel_attn): XLNetRelativeAttention(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ff): XLNetFeedForward(\n",
       "        (layer_norm): XLNetLayerNorm()\n",
       "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids)\n",
    "last_hidden_states = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4631,  2.5091,  1.6458,  ..., -0.1804, -0.0000, -0.4700],\n",
       "         [ 0.2742, -0.2042, -0.5669,  ...,  2.8734, -0.3431,  0.2065],\n",
       "         [ 0.9146,  0.5399,  0.0661,  ...,  1.4533,  0.4497, -0.0844],\n",
       "         ...,\n",
       "         [ 1.5695,  0.0000,  3.2084,  ...,  2.5973, -1.4548,  0.1073],\n",
       "         [ 1.3308,  1.4569, -0.1415,  ...,  1.9561, -1.5834,  0.5102],\n",
       "         [-1.2122, -0.2389, -0.1772,  ...,  0.4195, -0.1349, -0.2906]]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].detach()\n",
    "outputs[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XLNetForSequenceClassification(config)\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassification(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): XLNetLayerNorm()\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (sequence_summary): SequenceSummary(\n",
       "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "    (first_dropout): Identity()\n",
       "    (last_dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids, labels=labels)\n",
    "loss, logits = outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9764, grad_fn=<NllLossBackward>),\n",
       " tensor([[ 0.0759, -0.4278]], grad_fn=<AddmmBackward>),\n",
       " (None, None, None, None, None, None, None, None, None, None, None, None))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4727, 0.9764]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log_softmax(outputs[1].detach(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-en-2048-config.json from cache at /home/lyc/.torch/pytorch_transformers/063cbd65bb7d2e7fa034126477f72870c897d51a5e29a6baf2ebe35acf00810c.a9584498ff24d6bef104dcc2693a9efab757d2e5ad782c797c29c89fa445b552\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"asm\": false,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_index\": 0,\n",
      "  \"causal\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"emb_dim\": 2048,\n",
      "  \"embed_init_std\": 0.02209708691207961,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_index\": 1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"gelu_activation\": true,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_index\": 5,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"n_heads\": 16,\n",
      "  \"n_langs\": 1,\n",
      "  \"n_layers\": 12,\n",
      "  \"n_words\": 30145,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_index\": 2,\n",
      "  \"sinusoidal_embeddings\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"unk_index\": 3\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-en-2048-vocab.json from cache at /home/lyc/.torch/pytorch_transformers/6ce36d71d55c65bfdaee86d8ff7aa5118c45f98954b8893e0741c9ee6c61cd4a.dbe0e43202ae562bef29b0aaf02c97af23e2f0a2eb6100e071337d1f732be242\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-en-2048-merges.txt from cache at /home/lyc/.torch/pytorch_transformers/7a5b97f72ab092e8b0e2f268addeb588e41ca9fc9c78d8e9621de6e87cdfe787.992606ccc2e230e4b29e0a73a65a30dff361d6a3db3406289a6657c704fb0428\n"
     ]
    }
   ],
   "source": [
    "config = XLMConfig.from_pretrained('xlm-mlm-en-2048')\n",
    "tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')\n",
    "model = XLMForSequenceClassification(config)\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids, labels=labels)\n",
    "loss, logits = outputs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-config.json from cache at /home/lyc/.torch/pytorch_transformers/a27bb7c70e9002d7558d2682d5a95f3c0a8b31034616309459e0b51ef07ade09.f59b19eb0e361a0230a1106b66b8c6e7a994cb200cd63d9190cda8d56d75ff85\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"afn\": \"gelu\",\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 512,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 512,\n",
      "  \"n_special\": 0,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"token_ids\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"vocab_size\": 40478\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json from cache at /home/lyc/.torch/pytorch_transformers/4ab93d0cd78ae80e746c27c9cd34e90b470abdabe0590c9ec742df61625ba310.b9628f6fe5519626534b82ce7ec72b22ce0ae79550325f45c604a25c0ad87fd6\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt from cache at /home/lyc/.torch/pytorch_transformers/0f8de0dbd6a2bb6bde7d758f4c120dd6dd20b46f2bf0a47bc899c89f46532fde.20808570f9a3169212a577f819c845330da870aeb14c40f7319819fce10c3b76\n"
     ]
    }
   ],
   "source": [
    "config = OpenAIGPTConfig.from_pretrained('openai-gpt')\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "model = OpenAIGPTModel(config)\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIGPTModel(\n",
       "  (tokens_embed): Embedding(40478, 768)\n",
       "  (positions_embed): Embedding(512, 768)\n",
       "  (drop): Dropout(p=0.1)\n",
       "  (h): ModuleList(\n",
       "    (0): Block(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1)\n",
       "        (resid_dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_1): BertLayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (ln_2): BertLayerNorm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids)\n",
    "last_hidden_states = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.8225,  0.3185,  1.3487,  ..., -1.8885,  0.2741,  1.0912],\n",
       "          [-0.8317,  0.8078, -0.4801,  ..., -0.0639,  0.0191,  1.9491],\n",
       "          [-1.2643,  0.1680,  1.3317,  ..., -1.1416,  0.1503,  1.3841],\n",
       "          [-0.5614, -0.8596,  0.3522,  ...,  0.1271, -0.3651,  0.2104],\n",
       "          [ 0.5176,  2.5435,  0.3551,  ..., -0.5990, -0.7403,  0.4764],\n",
       "          [-0.1666,  0.7315,  1.0842,  ..., -0.7447, -1.6451, -1.4017]]],\n",
       "        grad_fn=<ViewBackward>),)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
