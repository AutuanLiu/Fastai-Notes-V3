{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# 多行输出\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hook\n",
    "- pytorch 的 hook 有 forwardhook和backward hook，必须包括三个参数 module，imput，output（当前模块，当前模块的输入，当前模块的输出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(10, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Linear(26, 10),\n",
    "        nn.Linear(10, 5),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(5, 3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3957, -0.4312, -0.2336],\n",
       "        [-0.4296, -0.3792, -0.2514],\n",
       "        [-0.4377, -0.3563, -0.2609],\n",
       "        [-0.4352, -0.5905, -0.0555],\n",
       "        [-0.4199, -0.3801, -0.2870],\n",
       "        [-0.4816, -0.4856, -0.2321],\n",
       "        [-0.4555, -0.4081, -0.3971],\n",
       "        [-0.4387, -0.3615, -0.2483],\n",
       "        [-0.3789, -0.5624, -0.0711],\n",
       "        [-0.5028, -0.6129, -0.3769]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_means = [[] for _ in model]\n",
    "act_stds  = [[] for _ in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_stats(i, mod, inp, outp):\n",
    "    if mod.training:\n",
    "        act_means[i].append(outp.data.mean())\n",
    "        act_stds [i].append(outp.data.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7faf65882f28>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7faf65882c88>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7faf65882a90>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7faf658824e0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, m in enumerate(model):\n",
    "    m.register_forward_hook(partial(append_stats, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3957, -0.4312, -0.2336],\n",
       "        [-0.4296, -0.3792, -0.2514],\n",
       "        [-0.4377, -0.3563, -0.2609],\n",
       "        [-0.4352, -0.5905, -0.0555],\n",
       "        [-0.4199, -0.3801, -0.2870],\n",
       "        [-0.4816, -0.4856, -0.2321],\n",
       "        [-0.4555, -0.4081, -0.3971],\n",
       "        [-0.4387, -0.3615, -0.2483],\n",
       "        [-0.3789, -0.5624, -0.0711],\n",
       "        [-0.5028, -0.6129, -0.3769]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(-0.0291)], [tensor(0.0751)], [tensor(0.1723)], [tensor(-0.3786)]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[tensor(0.5740)], [tensor(0.3283)], [tensor(0.2056)], [tensor(0.1304)]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_means\n",
    "act_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hook 在不使用的时候应该删除，否则内存会受不了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def children(m): return list(m.children())\n",
    "\n",
    "class Hook():\n",
    "    def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))\n",
    "    def remove(self): self.hook.remove()\n",
    "    def __del__(self): self.remove()\n",
    "\n",
    "def append_stats(hook, mod, inp, outp):\n",
    "    if not hasattr(hook,'stats'): hook.stats = ([],[])\n",
    "    means,stds = hook.stats\n",
    "    if mod.training:\n",
    "        means.append(outp.data.mean())\n",
    "        stds .append(outp.data.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks = [Hook(l, append_stats) for l in children(model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3957, -0.4312, -0.2336],\n",
       "        [-0.4296, -0.3792, -0.2514],\n",
       "        [-0.4377, -0.3563, -0.2609],\n",
       "        [-0.4352, -0.5905, -0.0555],\n",
       "        [-0.4199, -0.3801, -0.2870],\n",
       "        [-0.4816, -0.4856, -0.2321],\n",
       "        [-0.4555, -0.4081, -0.3971],\n",
       "        [-0.4387, -0.3615, -0.2483],\n",
       "        [-0.3789, -0.5624, -0.0711],\n",
       "        [-0.5028, -0.6129, -0.3769]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(-0.0291)], [tensor(0.5740)])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**给我们的Hooks类一个`__enter__` 和 `__exit__` 方法后，我们可以将它用作上下文管理器。这样可以确保一旦我们离开with块，所有的hook都被移除了。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "log1p(input, out=None) -> Tensor\n",
       "\n",
       "Returns a new tensor with the natural logarithm of (1 + :attr:`input`).\n",
       "\n",
       ".. math::\n",
       "    y_i = \\log_{e} (x_i + 1)\n",
       "\n",
       ".. note:: This function is more accurate than :func:`torch.log` for small\n",
       "          values of :attr:`input`\n",
       "\n",
       "Args:\n",
       "    input (Tensor): the input tensor\n",
       "    out (Tensor, optional): the output tensor\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> a = torch.randn(5)\n",
       "    >>> a\n",
       "    tensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])\n",
       "    >>> torch.log1p(a)\n",
       "    tensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.log1p??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropout\n",
    "**dropout 就是一个 mask 的过程**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torch import Tensor\n",
    "from typing import Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_mask(x:Tensor, sz:Collection[int], p:float):\n",
    "    \"Return a dropout mask of the same type as `x`, size `sz`, with probability `p` to cancel an element.\"\n",
    "    return x.new(*sz).bernoulli_(1-p).div_(1-p)\n",
    "\n",
    "class RNNDropout(nn.Module):\n",
    "    \"Dropout with probability `p` that is consistent on the seq_len dimension.\"\n",
    "\n",
    "    def __init__(self, p:float=0.5):\n",
    "        super().__init__()\n",
    "        self.p=p\n",
    "\n",
    "    def forward(self, x:Tensor)->Tensor:\n",
    "        if not self.training or self.p == 0.: return x\n",
    "        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n",
    "        return x * m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1863,  0.9396, -0.0000],\n",
       "         [-0.8098,  0.7910,  2.2062],\n",
       "         [-2.2199,  0.2210, -0.8520],\n",
       "         [ 0.0000, -0.0000, -0.5371]],\n",
       "\n",
       "        [[ 1.3177, -0.0863, -0.5836],\n",
       "         [-0.0000,  0.0000, -2.0321],\n",
       "         [-0.0000,  0.5223,  0.8653],\n",
       "         [ 0.8988, -1.0106, -0.3684]],\n",
       "\n",
       "        [[-0.1882,  0.7382, -1.1006],\n",
       "         [-1.7342,  0.0592,  0.0756],\n",
       "         [-1.0711, -0.0000, -1.6763],\n",
       "         [ 0.1145, -0.5760,  1.1264]]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3, 4, 3)\n",
    "b = a.clone()  # 这里必须是 clone， Python是链接\n",
    "a.bernoulli_(0.8).div_(0.8) * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1863,  0.9396, -2.3814],\n",
       "         [-0.8098,  0.7910,  2.2062],\n",
       "         [-2.2199,  0.2210, -0.8520],\n",
       "         [ 1.0485, -0.6884, -0.5371]],\n",
       "\n",
       "        [[ 1.3177, -0.0863, -0.5836],\n",
       "         [-0.2189,  0.9301, -2.0321],\n",
       "         [-1.3592,  0.5223,  0.8653],\n",
       "         [ 0.8988, -1.0106, -0.3684]],\n",
       "\n",
       "        [[-0.1882,  0.7382, -1.1006],\n",
       "         [-1.7342,  0.0592,  0.0756],\n",
       "         [-1.0711, -0.5491, -1.6763],\n",
       "         [ 0.1145, -0.5760,  1.1264]]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = RNNDropout(0.2)\n",
    "m(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 线性插值\n",
    "$$\\text{out}_i = \\text{start}_i + \\text{weight}_i \\times (\\text{end}_i - \\text{start}_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "lerp(start, end, weight, out=None)\n",
       "\n",
       "Does a linear interpolation of two tensors :attr:`start` and :attr:`end` based\n",
       "on a scalar or tensor :attr:`weight` and returns the resulting :attr:`out` tensor.\n",
       "\n",
       ".. math::\n",
       "    \\text{out}_i = \\text{start}_i + \\text{weight}_i \\times (\\text{end}_i - \\text{start}_i)\n",
       "\n",
       "The shapes of :attr:`start` and :attr:`end` must be\n",
       ":ref:`broadcastable <broadcasting-semantics>`. If :attr:`weight` is a tensor, then\n",
       "the shapes of :attr:`start`, :attr:`end` must be :ref:`broadcastable <broadcasting-semantics>`.\n",
       "\n",
       "Args:\n",
       "    start (Tensor): the tensor with the starting points\n",
       "    end (Tensor): the tensor with the ending points\n",
       "    weight (float or tensor): the weight for the interpolation formula\n",
       "    out (Tensor, optional): the output tensor\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> start = torch.arange(1., 5.)\n",
       "    >>> end = torch.empty(4).fill_(10)\n",
       "    >>> start\n",
       "    tensor([ 1.,  2.,  3.,  4.])\n",
       "    >>> end\n",
       "    tensor([ 10.,  10.,  10.,  10.])\n",
       "    >>> torch.lerp(start, end, 0.5)\n",
       "    tensor([ 5.5000,  6.0000,  6.5000,  7.0000])\n",
       "    >>> torch.lerp(start, end, torch.full_like(start, 0.5))\n",
       "    tensor([ 5.5000,  6.0000,  6.5000,  7.0000])\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.lerp??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BatchNorma 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, nf, mom=0.1, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # NB: pytorch bn mom is opposite of what you'd expect\n",
    "        self.mom,self.eps = mom,eps\n",
    "        self.mults = nn.Parameter(torch.ones (nf,1,1))\n",
    "        self.adds  = nn.Parameter(torch.zeros(nf,1,1))\n",
    "        self.register_buffer('vars',  torch.ones(1,nf,1,1))\n",
    "        self.register_buffer('means', torch.zeros(1,nf,1,1))\n",
    "\n",
    "    def update_stats(self, x):\n",
    "        m = x.mean((0,2,3), keepdim=True)\n",
    "        v = x.var ((0,2,3), keepdim=True)\n",
    "        self.means.lerp_(m, self.mom)\n",
    "        self.vars.lerp_ (v, self.mom)\n",
    "        return m,v\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            with torch.no_grad(): m,v = self.update_stats(x)\n",
    "        else: m,v = self.means,self.vars\n",
    "        x = (x-m) / (v+self.eps).sqrt()\n",
    "        return x*self.mults + self.adds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    __constants__ = ['eps']\n",
    "    def __init__(self, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.mult = nn.Parameter(tensor(1.))\n",
    "        self.add  = nn.Parameter(tensor(0.))\n",
    "\n",
    "    def forward(self, x):\n",
    "        m = x.mean((1,2,3), keepdim=True)\n",
    "        v = x.var ((1,2,3), keepdim=True)\n",
    "        x = (x-m) / ((v+self.eps).sqrt())\n",
    "        return x*self.mult + self.add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceNorm(nn.Module):\n",
    "    __constants__ = ['eps']\n",
    "    def __init__(self, nf, eps=1e-0):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.mults = nn.Parameter(torch.ones (nf,1,1))\n",
    "        self.adds  = nn.Parameter(torch.zeros(nf,1,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        m = x.mean((2,3), keepdim=True)\n",
    "        v = x.var ((2,3), keepdim=True)\n",
    "        res = (x-m) / ((v+self.eps).sqrt())\n",
    "        return res*self.mults + self.adds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batchnorm 会收到 batchsize的影响，当bs非常小的时候，均值几乎接近0\n",
    "- 使用 光滑 BN 解决"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningBatchNorm(nn.Module):\n",
    "    def __init__(self, nf, mom=0.1, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.mom,self.eps = mom,eps\n",
    "        self.mults = nn.Parameter(torch.ones (nf,1,1))\n",
    "        self.adds = nn.Parameter(torch.zeros(nf,1,1))\n",
    "        self.register_buffer('sums', torch.zeros(1,nf,1,1))\n",
    "        self.register_buffer('sqrs', torch.zeros(1,nf,1,1))\n",
    "        self.register_buffer('batch', tensor(0.))\n",
    "        self.register_buffer('count', tensor(0.))\n",
    "        self.register_buffer('step', tensor(0.))\n",
    "        self.register_buffer('dbias', tensor(0.))\n",
    "\n",
    "    def update_stats(self, x):\n",
    "        bs,nc,*_ = x.shape\n",
    "        self.sums.detach_()\n",
    "        self.sqrs.detach_()\n",
    "        dims = (0,2,3)\n",
    "        s = x.sum(dims, keepdim=True)\n",
    "        ss = (x*x).sum(dims, keepdim=True)\n",
    "        c = self.count.new_tensor(x.numel()/nc)\n",
    "        mom1 = 1 - (1-self.mom)/math.sqrt(bs-1)\n",
    "        self.mom1 = self.dbias.new_tensor(mom1)\n",
    "        self.sums.lerp_(s, self.mom1)\n",
    "        self.sqrs.lerp_(ss, self.mom1)\n",
    "        self.count.lerp_(c, self.mom1)\n",
    "        self.dbias = self.dbias*(1-self.mom1) + self.mom1\n",
    "        self.batch += bs\n",
    "        self.step += 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training: self.update_stats(x)\n",
    "        sums = self.sums\n",
    "        sqrs = self.sqrs\n",
    "        c = self.count\n",
    "        if self.step<100:\n",
    "            sums = sums / self.dbias\n",
    "            sqrs = sqrs / self.dbias\n",
    "            c    = c    / self.dbias\n",
    "        means = sums/c\n",
    "        vars = (sqrs/c).sub_(means*means)\n",
    "        if bool(self.batch < 20): vars.clamp_min_(0.01)\n",
    "        x = (x-means).div_((vars.add_(self.eps)).sqrt())\n",
    "        return x.mul_(self.mults).add_(self.adds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified RunningBatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningBatchNorm(nn.Module):\n",
    "    def __init__(self, nf, mom=0.1, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.mom, self.eps = mom, eps\n",
    "        self.mults = nn.Parameter(torch.ones (nf,1,1))\n",
    "        self.adds  = nn.Parameter(torch.zeros(nf,1,1))\n",
    "        self.register_buffer('sums', torch.zeros(1,nf,1,1))\n",
    "        self.register_buffer('sqrs', torch.zeros(1,nf,1,1))\n",
    "        self.register_buffer('count', tensor(0.))\n",
    "        self.register_buffer('factor', tensor(0.))\n",
    "        self.register_buffer('offset', tensor(0.))\n",
    "        self.batch = 0\n",
    "        \n",
    "    def update_stats(self, x):\n",
    "        bs,nc,*_ = x.shape\n",
    "        self.sums.detach_()\n",
    "        self.sqrs.detach_()\n",
    "        dims = (0,2,3)\n",
    "        s    = x    .sum(dims, keepdim=True)\n",
    "        ss   = (x*x).sum(dims, keepdim=True)\n",
    "        c    = s.new_tensor(x.numel()/nc)\n",
    "        mom1 = s.new_tensor(1 - (1-self.mom)/math.sqrt(bs-1))\n",
    "        self.sums .lerp_(s , mom1)\n",
    "        self.sqrs .lerp_(ss, mom1)\n",
    "        self.count.lerp_(c , mom1)\n",
    "        self.batch += bs\n",
    "        means = self.sums/self.count\n",
    "        varns = (self.sqrs/self.count).sub_(means*means)\n",
    "        if bool(self.batch < 20): varns.clamp_min_(0.01)\n",
    "        self.factor = self.mults / (varns+self.eps).sqrt()\n",
    "        self.offset = self.adds - means*self.factor\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training: self.update_stats(x)\n",
    "        return x*self.factor + self.offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
