{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# 多行输出\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc\n",
    "import torch\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader\n",
    "from torch_geometric.nn import GraphConv, TopKPooling, GatedGraphConv, SAGEConv, SGConv\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YooChooseBinaryDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['click_binary100m.pt']\n",
    "        # return ['click_binary.pt']    # for whole dataset\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        # clicks = pd.read_csv(root / 'clicks_pro.csv', encoding='utf-8', low_memory=False)  # 使用全部数据\n",
    "        clicks = pd.read_csv(root / 'clicks_pro_100m.csv', encoding='utf-8', low_memory=False)    # 使用部分数据\n",
    "\n",
    "        # process by session_id\n",
    "        grouped = iter(clicks.groupby('session_id'))\n",
    "        lens = clicks.session_id.unique().shape[0]\n",
    "\n",
    "        # clicks 不需要常驻内存, 并且使用动态加载\n",
    "        del clicks\n",
    "\n",
    "        for session_id, group in tqdm(grouped, total=lens):\n",
    "            # 重新编码，作为节点的顺序\n",
    "            sess_item_id = LabelEncoder().fit_transform(group.item_id)\n",
    "\n",
    "            # 重建索引\n",
    "            group = group.reset_index(drop=True)\n",
    "            group['sess_item_id'] = sess_item_id\n",
    "\n",
    "            # 节点的初始特征\n",
    "            # 重复的浏览记录当做一次记录\n",
    "            # 使用 item id 作为节点特征\n",
    "            # 每个子表 group 都有同样的 session id\n",
    "            node_features = group.loc[group.session_id == session_id, ['sess_item_id', 'item_id']].sort_values(\n",
    "                'sess_item_id').item_id.drop_duplicates().values\n",
    "            node_features = torch.LongTensor(node_features).unsqueeze(1)\n",
    "\n",
    "            # 序列访问的顺序\n",
    "            source_nodes = group.sess_item_id.values[:-1]\n",
    "            target_nodes = group.sess_item_id.values[1:]\n",
    "            edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n",
    "\n",
    "            x = node_features    # item_id , 可以考虑使用 category\n",
    "            y = torch.FloatTensor([group.label.values[0]])    # 对graph进行二分类，确定在该session下是否有购买行为\n",
    "\n",
    "            # 每个 session 当做一个 graph\n",
    "            data = Data(x=x, edge_index=edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, sparse_sz, emb_sz=128, p=0.5):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.item_embedding = nn.Embedding(num_embeddings=sparse_sz, embedding_dim=emb_sz)\n",
    "\n",
    "        self.conv1 = SAGEConv(emb_sz, 128)\n",
    "        self.pool1 = TopKPooling(128, ratio=0.8)\n",
    "\n",
    "        self.conv2 = SAGEConv(128, 128)\n",
    "        self.pool2 = TopKPooling(128, ratio=0.8)\n",
    "\n",
    "        self.conv3 = SAGEConv(128, 128)\n",
    "        self.pool3 = TopKPooling(128, ratio=0.8)\n",
    "\n",
    "        self.fc1 = nn.Sequential(nn.Linear(256, 128), nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(nn.Linear(128, 64), nn.ReLU())\n",
    "        self.linear = nn.Linear(64, 1)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.item_embedding(x).squeeze(1)\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, *_ = self.pool1(x, edge_index, batch=batch)\n",
    "        x1 = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x, edge_index, _, batch, *_ = self.pool2(x, edge_index, batch=batch)\n",
    "        x2 = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x, edge_index, _, batch, *_ = self.pool3(x, edge_index, batch=batch)\n",
    "        x3 = torch.cat([global_max_pool(x, batch), global_mean_pool(x, batch)], dim=1)\n",
    "\n",
    "        x = x1 + x2 + x3\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = torch.sigmoid(self.linear(x)).squeeze(1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        label = data.y.to(device)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data).detach().cpu().numpy()\n",
    "        label = data.y.detach().cpu().numpy()\n",
    "        predictions.append(pred)\n",
    "        labels.append(label)\n",
    "    predictions = np.hstack(predictions)\n",
    "    labels = np.hstack(labels)\n",
    "    return roc_auc_score(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #  各种设置\n",
    "lr, bs, ps = 0.001, 512, 0.5\n",
    "sparse_sz, emb_sz = 37495, 128    # sparse_sz = clicks.item_id.max() + 1\n",
    "# sparse_sz, emb_sz = 48256, 128    # for whole dataset\n",
    "num_epochs = 10\n",
    "root = Path('../data/yoochoose-data/')\n",
    "# device = torch.device('cuda: 0' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device( 'cpu')\n",
    "model = Net(sparse_sz, emb_sz=emb_sz, p=ps).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 数据集相关\n",
    "dataset = YooChooseBinaryDataset(root=root)\n",
    "dataset = dataset.shuffle()\n",
    "train_dataset, val_dataset, test_dataset = dataset[:800000], dataset[800000:900000], dataset[900000:]\n",
    "\n",
    "# loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs)\n",
    "val_loader = DataLoader(val_dataset, batch_size=bs)\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs)\n",
    "del dataset, train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    loss = train()    # 训练\n",
    "\n",
    "    # 评估\n",
    "    train_auc = evaluate(train_loader)\n",
    "    val_auc = evaluate(val_loader)\n",
    "    test_auc = evaluate(test_loader)\n",
    "    print(\n",
    "        f'Epoch: {epoch:03d}, Loss: {loss:.5f}, Train Auc: {train_auc:.5f}, Val Auc: {val_auc:.5f}, Test Auc: {test_auc:.5f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
