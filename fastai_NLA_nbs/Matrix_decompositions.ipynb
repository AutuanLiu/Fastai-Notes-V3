{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# 多行输出\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Decompositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 矩阵分解作用\n",
    "    - 矩阵填充(通过矩阵分解来填充原有矩阵，例如协同过滤的ALS算法就是填充原有矩阵)\n",
    "    - 清理异常值与离群点\n",
    "    - 降维、压缩\n",
    "    - 个性化推荐\n",
    "    - 间接的特征组合(计算特征间相似度)\n",
    "    - 聚类分析、数据预处理、低维度特征学习、特征学习、大数据分析\n",
    "- 矩阵分解的方法\n",
    "特征值分解。\n",
    "    - PCA(Principal Component Analysis)分解，作用：降维、压缩。\n",
    "    - SVD(Singular Value Decomposition)分解，也叫奇异值分解。\n",
    "    - LSI(Latent Semantic Indexing)或者叫LSA(Latent Semantic Analysis)，隐语义分析分解。\n",
    "    - PLSA(Probabilistic Latent Semantic Analysis)，概率潜在语义分析。PLSA和LDA都是主题模型，PLSA是判别模型。\n",
    "    - NMF(Non-negative Matrix Factorization)，非负矩阵分解。非负矩阵分解能够广泛应用于图像分析、文本挖掘和语言处理等领域。\n",
    "    - LDA\n",
    "- 特征值分解\n",
    "    对于矩阵A，有一组特征向量v，将这组向量进行正交化单位化，就能得到一组正交单位向量。特征值分解，就是将矩阵A分解为如下式：$$A=Q\\sum Q^{-1}$$\n",
    "    其中，Q是矩阵A的特征向量组成的矩阵，则是一个*对角阵，对角线上的元素就是特征值*\n",
    "\n",
    "特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有**多么重要**，而特征向量表示这个特征**是什么**，\n",
    "\n",
    "特征值分解也有很多的局限，比如说变换的矩阵**必须是方阵**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "奇异值分解是一个能适用于**任意矩阵**的一种分解的方法，对于任意矩阵A总是存在一个奇异值分解：\n",
    "$$A=U\\sum V^T$$\n",
    "假设$A$是一个$m*n$的矩阵，那么得到的$U$是一个$m*m$的方阵，$U$里面的正交向量被称为左奇异向量。$\\sum$是一个 $m*n$ 的矩阵，$\\sum$ 除了对角线其它元素都为 0，对角线上的元素称为**奇异值**。$v^T$ 是 $V$ 的转置矩阵，是一个 $n*n$ 的矩阵，它里面的正交向量被称为右奇异值向量。而且一般来讲，我们会将Σ上的值按从大到小的顺序排列。\n",
    "\n",
    "$A^T A$ 的特征向量即右奇异向量，$A A^T$ 的特征向量即左奇异向量，奇异值是 $\\sqrt{\\lambda}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维\n",
    "![NMF and SVD](../imgs/PCA1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 中心化 $ X-\\bar{X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于特征值分解协方差矩阵实现PCA算法\n",
    "输入：数据集 $X=\\left\\{ x_{1},x_{2},x_{3},...,x_{n} \\right\\}$ ，需要降到$k$维。\n",
    "    1. 去平均值(即去中心化)，即每一位特征减去各自的平均值 $ X-\\bar{X}$\n",
    "    2. 计算协方差矩阵 $\\frac{1}{n}XX^T$, 这里除或不除样本数量 n 或 n-1,其实对求出的特征向量没有影响\n",
    "    3. 用特征值分解方法求协方差矩阵$\\frac{1}{n}XX^T$ 的特征值与特征向量\n",
    "    4. 对特征值从大到小排序，选择其中最大的k个。然后将其对应的$k$个特征向量分别作为行向量组成特征向量矩阵 $P$\n",
    "    5. 将数据转换到$k$个特征向量构建的新空间中，即Y=PX。\n",
    "输出 Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF and SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NMF and SVD](NMF_and_SVD.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
